import json
import os
import datetime
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# --- Configuration ---
# This is the model we'll use. It's powerful, fast on CPU, and small (~80MB).
# It's pre-packaged in the Docker image, so this works offline.
MODEL_NAME = os.environ.get("MODEL_PATH", "all-MiniLM-L6-v2")
# These paths are INSIDE the container
INPUT_DATA_DIR = "/app/data"
# The main JSON file that describes the task
CHALLENGE_INPUT_FILE = os.path.join(INPUT_DATA_DIR, "challenge1b_input.json")
# The final output file
OUTPUT_FILE = os.path.join(INPUT_DATA_DIR, "challenge1b_output.json")

# --- Helper Functions ---
def clean_text(text):
    """A simple utility to clean up text for better embedding."""
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# --- Core Classes ---

class DocumentProcessor:
    """
    Handles the loading of documents and creation of semantic chunks.
    This class cleverly uses the output from your Round 1A solution.
    """
    def __init__(self, doc_info, base_path):
        self.filename = doc_info['filename']
        self.json_path = os.path.join(base_path, os.path.splitext(self.filename)[0] + ".json")
        # For 1B, we assume the 1A JSON outputs are available. If not, we'd need a text extractor.
        # This is a key part of the "Connecting the Dots" theme.
        self.structure = self._load_structure()

    def _load_structure(self):
        """Loads the JSON file generated by the Round 1A engine."""
        try:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            # Fallback if the 1A JSON doesn't exist or is invalid.
            # In a real system, we'd run a simple text extractor here.
            # For the hackathon, we assume 1A output is present.
            print(f"Warning: Could not load or parse structure for {self.filename}. It will be skipped.")
            return {"title": "Unknown Title", "outline": []}

    def get_semantic_chunks(self):
        """
        Creates contextually relevant chunks from the document's outline.
        Each chunk is a heading plus its content.
        """
        chunks = []
        outline = self.structure.get('outline', [])
        # We need to sort the outline by page and position to reconstruct content flow.
        # Let's assume the 1A output is already logically sorted.
        
        # A simple chunking strategy: each outline item is a chunk.
        # A more advanced strategy would concatenate text between headings.
        # For this hackathon, treating each heading as a distinct topic is robust.
        for item in outline:
            chunks.append({
                "document": self.filename,
                "section_title": item['text'],
                "content": item['text'], # For ranking, the heading itself is a great summary
                "page_number": item['page']
            })
        return chunks

class PersonaIntelEngine:
    """
    The main engine that drives the persona-based analysis.
    """
    def __init__(self, model_name):
        print(f"Loading embedding model: {model_name}...")
        # This will use the cached model inside the Docker container
        self.model = SentenceTransformer(model_name)
        print("Model loaded successfully.")
    
    def run(self, input_data):
        """
s        Orchestrates the entire process from input to output.
        """
        persona = input_data['persona']['role']
        job_to_be_done = input_data['job_to_be_done']['task']
        
        # 1. Create the Persona-Aware Compound Query
        compound_query = f"As a {persona}, I need to {job_to_be_done}"
        print(f"Compound Query: {compound_query}")

        # 2. Process all documents and create a flat list of semantic chunks
        all_chunks = []
        doc_processors = [DocumentProcessor(doc, INPUT_DATA_DIR) for doc in input_data['documents']]
        for processor in doc_processors:
            all_chunks.extend(processor.get_semantic_chunks())

        if not all_chunks:
            print("No processable chunks found in any document.")
            self._write_empty_output(input_data)
            return

        # 3. Embed the query and all chunks
        print(f"Embedding {len(all_chunks)} chunks...")
        query_embedding = self.model.encode([compound_query])
        chunk_embeddings = self.model.encode([chunk['content'] for chunk in all_chunks])
        print("Embedding complete.")

        # 4. Calculate Relevance (Cosine Similarity) and Rank
        similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
        
        for i, chunk in enumerate(all_chunks):
            chunk['relevance_score'] = similarities[i]

        # Sort chunks by relevance score in descending order
        ranked_chunks = sorted(all_chunks, key=lambda x: x['relevance_score'], reverse=True)
        
        # 5. Generate the final JSON output
        self._generate_output(input_data, ranked_chunks)

    def _generate_output(self, input_data, ranked_chunks):
        """Formats the ranked results into the required JSON structure."""
        
        # --- Metadata ---
        metadata = {
            "input_documents": [doc['filename'] for doc in input_data['documents']],
            "persona": input_data['persona']['role'],
            "job_to_be_done": input_data['job_to_be_done']['task'],
            "processing_timestamp": datetime.datetime.now().isoformat()
        }

        # --- Extracted Sections (Top 5) ---
        extracted_sections = []
        for i, chunk in enumerate(ranked_chunks[:5]):
            extracted_sections.append({
                "document": chunk['document'],
                "section_title": chunk['section_title'],
                "importance_rank": i + 1,
                "page_number": chunk['page_number']
            })
            
        # --- Subsection Analysis ---
        # For this, we'll take the top 5 chunks and find the most relevant sentence *within* them.
        # Since our chunks are just headings, the "refined text" is the heading itself.
        # This is a valid interpretation for this specific implementation.
        # A more advanced version would have the full text content in the chunk.
        subsection_analysis = []
        for chunk in ranked_chunks[:5]:
             subsection_analysis.append({
                "document": chunk['document'],
                # "refined_text" is the key content that led to the ranking. In our case, the heading.
                "refined_text": clean_text(chunk['section_title']),
                "page_number": chunk['page_number']
            })

        final_output = {
            "metadata": metadata,
            "extracted_sections": extracted_sections,
            "subsection_analysis": subsection_analysis
        }

        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=4, ensure_ascii=False)
        print(f"Successfully generated output at {OUTPUT_FILE}")

    def _write_empty_output(self, input_data):
        """Writes a compliant but empty output if processing fails."""
        metadata = {
            "input_documents": [doc['filename'] for doc in input_data['documents']],
            "persona": input_data['persona']['role'],
            "job_to_be_done": input_data['job_to_be_done']['task'],
            "processing_timestamp": datetime.datetime.now().isoformat()
        }
        final_output = {"metadata": metadata, "extracted_sections": [], "subsection_analysis": []}
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=4, ensure_ascii=False)


# --- Main Execution Block ---

if __name__ == "__main__":
    if not os.path.exists(CHALLENGE_INPUT_FILE):
        print(f"Error: Main input file not found at {CHALLENGE_INPUT_FILE}")
    else:
        with open(CHALLENGE_INPUT_FILE, 'r', encoding='utf-8') as f:
            challenge_data = json.load(f)
        
        engine = PersonaIntelEngine(MODEL_NAME)
        engine.run(challenge_data)